{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import mlflow\n",
    "import logging\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/20 07:45:12 INFO mlflow.tracking.fluent: Experiment with name 'wildfire-prediction-test' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "# In your imports section, add:\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "os.makedirs(\"./mlflow\", exist_ok=True)\n",
    "os.makedirs(\"./mlflow/.trash\", exist_ok=True)\n",
    "\n",
    "# Set tracking URI to a local directory\n",
    "mlflow.set_tracking_uri(\"file:./mlflow\")\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"wildfire-prediction-test\")\n",
    "\n",
    "# In your training function, replace the MLflow section with:\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    # Create and train the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame(\n",
    "        model.feature_importances_,\n",
    "        index=X_train.columns,\n",
    "        columns=['importance']\n",
    "    ).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save feature importance to a proper location\n",
    "    os.makedirs(\"models/feature_importance\", exist_ok=True)\n",
    "    feature_importance.to_csv(\"models/feature_importance/feature_importance.csv\")\n",
    "    \n",
    "    # Log with MLflow\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"n_estimators\", model.n_estimators)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred, average='weighted'))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, y_pred, average='weighted'))\n",
    "        mlflow.log_metric(\"f1\", f1_score(y_test, y_pred, average='weighted'))\n",
    "        \n",
    "        # Log feature importance\n",
    "        mlflow.log_artifact(\"models/feature_importance/feature_importance.csv\")\n",
    "        \n",
    "        # Log model with signature\n",
    "        signature = infer_signature(X_train, y_pred)\n",
    "        mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "        \n",
    "        # Save model locally\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        mlflow.sklearn.save_model(model, \"models/wildfire_model\")\n",
    "    \n",
    "    return model, feature_importance\n",
    "\n",
    "# Your prediction function can remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Load the merged dataset from JSON file.\"\"\"\n",
    "    logger.info(f\"Loading data from {data_path}\")\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess the data for training.\"\"\"\n",
    "    logger.info(\"Preprocessing data\")\n",
    "    \n",
    "    # Create an empty list to hold the flattened records\n",
    "    flattened_records = []\n",
    "    \n",
    "    # Process each record\n",
    "    for record in data:\n",
    "        # Skip records with missing weather or vegetation data\n",
    "        if record['weather'] is None or record['vegetation'] is None:\n",
    "            continue\n",
    "            \n",
    "        # Create a flattened dictionary\n",
    "        flat_record = {\n",
    "            # Event features\n",
    "            'lat': float(record['event']['lat']),\n",
    "            'lon': float(record['event']['lon']),\n",
    "            'brightness': float(record['event']['brightness']),\n",
    "            'scan': float(record['event']['scan']),\n",
    "            'track': float(record['event']['track']),\n",
    "            'confidence': float(record['event']['confidence']),\n",
    "            'bright_t31': float(record['event']['bright_t31']),\n",
    "            'frp': float(record['event']['frp']),\n",
    "            'daynight': 1 if record['event']['daynight'] == \"D\" else 0,  # Day=1, Night=0\n",
    "            \n",
    "            # Weather features\n",
    "            'tavg': record['weather'].get('tavg'),\n",
    "            'tmin': record['weather'].get('tmin'),\n",
    "            'tmax': record['weather'].get('tmax'),\n",
    "            'prcp': record['weather'].get('prcp'),\n",
    "            'wspd': record['weather'].get('wspd'),\n",
    "            'pres': record['weather'].get('pres'),\n",
    "            \n",
    "            # Vegetation features\n",
    "            'ndvi': record['vegetation'].get('ndvi'),\n",
    "            'evi': record['vegetation'].get('evi'),\n",
    "            \n",
    "            # Target: high severity fire (confidence > 80 and frp > 20)\n",
    "            'high_severity': 1 if (float(record['event']['confidence']) > 80 and \n",
    "                                float(record['event']['frp']) > 20) else 0\n",
    "        }\n",
    "        \n",
    "        flattened_records.append(flat_record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(flattened_records)\n",
    "    \n",
    "    # Replace NaN values with median for each column\n",
    "    for col in df.columns:\n",
    "        if col != 'high_severity' and df[col].dtype in [np.float64, np.int64]:\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    logger.info(f\"Preprocessed data shape: {df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Train a random forest model on the preprocessed data with MLflow tracking.\"\"\"\n",
    "    logger.info(\"Training model\")\n",
    "    \n",
    "    # Define features and target\n",
    "    features = [col for col in df.columns if col != 'high_severity']\n",
    "    X = df[features]\n",
    "    y = df['high_severity']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Initialize MLflow run\n",
    "    with mlflow.start_run(run_name=\"random-forest-wildfire\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"test_size\", test_size)\n",
    "        mlflow.log_param(\"random_state\", random_state)\n",
    "        mlflow.log_param(\"n_estimators\", 100)\n",
    "        mlflow.log_param(\"max_depth\", 6)\n",
    "        mlflow.log_param(\"max_features\", 3)\n",
    "        mlflow.log_param(\"features\", features)\n",
    "        \n",
    "        # Train model with progress bar\n",
    "        print(\"Training Random Forest model...\")\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            max_features=3,\n",
    "            random_state=random_state,\n",
    "            verbose=0  # Keep sklearn's verbose off as we're using tqdm\n",
    "        )\n",
    "        \n",
    "        # Use a progress bar for fitting\n",
    "        with tqdm(total=100, desc=\"Training Progress\") as pbar:\n",
    "            # Train model with fallback for older sklearn versions\n",
    "            try:\n",
    "                model.fit(X_train, y_train, callback=lambda _, i, __: pbar.update(1))\n",
    "            except TypeError:\n",
    "                model.fit(X_train, y_train)\n",
    "                for i in range(0, 100, 10):\n",
    "                    pbar.update(10)\n",
    "                    \n",
    "        # Evaluate model with progress bar\n",
    "        print(\"Evaluating model...\")\n",
    "        with tqdm(total=len(X_test), desc=\"Evaluation Progress\") as pbar:\n",
    "            y_pred = []\n",
    "            batch_size = max(1, len(X_test) // 10)\n",
    "            \n",
    "            for i in range(0, len(X_test), batch_size):\n",
    "                end = min(i + batch_size, len(X_test))\n",
    "                batch_pred = model.predict(X_test[i:end])\n",
    "                y_pred.extend(batch_pred)\n",
    "                pbar.update(end - i)\n",
    "            \n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "        # Extract and log individual metrics from classification report\n",
    "        report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "        for label, metrics in report_dict.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric_name, value in metrics.items():\n",
    "                    mlflow.log_metric(f\"{label}_{metric_name}\", value)\n",
    "        \n",
    "        # Log feature importance\n",
    "        feature_importances = model.feature_importances_\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': feature_importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Create proper directories for artifacts\n",
    "        artifacts_dir = \"../artifacts/feature_importance\"\n",
    "        os.makedirs(artifacts_dir, exist_ok=True)\n",
    "        \n",
    "        # Save feature importance CSV to a proper location with timestamp\n",
    "        from datetime import datetime\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        importance_path = f\"{artifacts_dir}/feature_importance_{timestamp}.csv\"\n",
    "        importance_df.to_csv(importance_path, index=False)\n",
    "        \n",
    "        # Log the feature importance file as an MLflow artifact\n",
    "        mlflow.log_artifact(importance_path)\n",
    "        \n",
    "        # Create a visualization of feature importance and save it\n",
    "        plt_path = f\"{artifacts_dir}/feature_importance_{timestamp}.png\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(importance_df['Feature'][:10], importance_df['Importance'][:10])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.title('Top 10 Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(plt_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Log the plot as an artifact\n",
    "            mlflow.log_artifact(plt_path)\n",
    "        except ImportError:\n",
    "            logger.warning(\"Matplotlib not installed. Skipping feature importance visualization.\")\n",
    "        \n",
    "        # Create model signature for proper schema tracking\n",
    "        signature = infer_signature(X_train, y_pred)\n",
    "        \n",
    "        # Log the model with its signature\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            \"random_forest_model\",\n",
    "            signature=signature,\n",
    "            input_example=X_train.iloc[0].to_dict()\n",
    "        )\n",
    "        \n",
    "        # Also save model to disk outside of MLflow\n",
    "        output_dir = \"../models\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(f\"{output_dir}/wildfire_model_{timestamp}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        # Also save the latest version with a fixed name for easy access\n",
    "        with open(f\"{output_dir}/wildfire_model_latest.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        logger.info(f\"Model saved to {output_dir}/wildfire_model_{timestamp}.pkl\")\n",
    "        logger.info(f\"Model also saved as {output_dir}/wildfire_model_latest.pkl\")\n",
    "        logger.info(f\"Model logged to MLflow with run_id: {mlflow.active_run().info.run_id}\")\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26198a0611bc4fc88f3e047ea5330383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56c44f89b744b2bbbf07c3f8f8e3ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the model training pipeline with MLflow tracking.\"\"\"\n",
    "    logger.info(\"Starting wildfire prediction model training\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"../data/merged_complete.json\"\n",
    "    data = load_data(data_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df = preprocess_data(data)\n",
    "    \n",
    "    # Train model\n",
    "    model, accuracy = train_model(df)\n",
    "    \n",
    "    logger.info(f\"Wildfire prediction model training completed with accuracy: {accuracy:.4f}\")\n",
    "    logger.info(f\"View experiments at: http://localhost:5000 (after starting mlflow ui)\")\n",
    "    \n",
    "    return model, accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model from a pickle file.\"\"\"\n",
    "    logger.info(f\"Loading model from {model_path}\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data):\n",
    "    \"\"\"Prepare input data for prediction.\"\"\"\n",
    "    # If data is a single record, convert to list\n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "    \n",
    "    # Create an empty list to hold the flattened records\n",
    "    flattened_records = []\n",
    "    \n",
    "    # Process each record\n",
    "    for record in data:\n",
    "        # Handle cases where weather or vegetation is None\n",
    "        weather = record.get('weather') or {}\n",
    "        vegetation = record.get('vegetation') or {}\n",
    "        \n",
    "        # Create a flattened dictionary\n",
    "        flat_record = {\n",
    "            # Event features\n",
    "            'lat': float(record['event']['lat']),\n",
    "            'lon': float(record['event']['lon']),\n",
    "            'brightness': float(record['event']['brightness']),\n",
    "            'scan': float(record['event']['scan']),\n",
    "            'track': float(record['event']['track']),\n",
    "            'confidence': float(record['event']['confidence']),\n",
    "            'bright_t31': float(record['event']['bright_t31']),\n",
    "            'frp': float(record['event']['frp']),\n",
    "            'daynight': 1 if record['event']['daynight'] == \"D\" else 0,  # Day=1, Night=0\n",
    "            \n",
    "            # Weather features (using default values if missing)\n",
    "            'tavg': weather.get('tavg', 25.0),\n",
    "            'tmin': weather.get('tmin', 20.0),\n",
    "            'tmax': weather.get('tmax', 30.0),\n",
    "            'prcp': weather.get('prcp', 0.0),\n",
    "            'wspd': weather.get('wspd', 10.0),\n",
    "            'pres': weather.get('pres', 1010.0),\n",
    "            \n",
    "            # Vegetation features (using default values if missing)\n",
    "            'ndvi': vegetation.get('ndvi', 0.5),\n",
    "            'evi': vegetation.get('evi', 0.4),\n",
    "        }\n",
    "        \n",
    "        flattened_records.append(flat_record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(flattened_records)\n",
    "    \n",
    "    # Replace NaN values with median for each column\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in [np.float64, np.int64]:\n",
    "            df[col] = df[col].fillna(df[col].median() if not df[col].isnull().all() else 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_data):\n",
    "    \"\"\"Make predictions using the trained model.\"\"\"\n",
    "    logger.info(\"Making predictions\")\n",
    "    \n",
    "    # Prepare input data\n",
    "    df = prepare_input(input_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(df)\n",
    "    probabilities = model.predict_proba(df)[:, 1]  # Probability of high severity\n",
    "    \n",
    "    # Create results\n",
    "    results = []\n",
    "    for i, record in enumerate(input_data):\n",
    "        result = {\n",
    "            'lat': record['event']['lat'],\n",
    "            'lon': record['event']['lon'],\n",
    "            'date': record['event']['date'],\n",
    "            'high_severity': bool(predictions[i]),\n",
    "            'probability': float(probabilities[i]),\n",
    "            'frp': float(record['event']['frp']),\n",
    "            'confidence': float(record['event']['confidence'])\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11ceaf954374964ac7734db9de26f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prediction Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample prediction results:\n",
      "Record 1: High Severity = False (Probability: 0.17)\n",
      "Record 2: High Severity = False (Probability: 0.00)\n",
      "Record 3: High Severity = False (Probability: 0.00)\n",
      "Record 4: High Severity = False (Probability: 0.00)\n",
      "Record 5: High Severity = False (Probability: 0.04)\n"
     ]
    }
   ],
   "source": [
    "# Replace the prediction execution cell with this version\n",
    "# This version works in a notebook environment without requiring command line arguments\n",
    "\n",
    "def run_predictions(input_path=\"../data/merged_complete.json\", \n",
    "                   output_path=\"../predictions/wildfire_predictions.json\",\n",
    "                   model_path=\"../models/wildfire_model_latest.pkl\"):\n",
    "    \"\"\"Run predictions from within the notebook environment\"\"\"\n",
    "    logger.info(f\"Starting prediction process with input: {input_path}\")\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        # Load input data\n",
    "        with open(input_path, 'r') as f:\n",
    "            input_data = json.load(f)\n",
    "        \n",
    "        # Take a subset for testing if the input is large\n",
    "        test_data = input_data[:10]  # Use first 10 records\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions...\")\n",
    "        with tqdm(total=len(test_data), desc=\"Prediction Progress\") as pbar:\n",
    "            results = predict(model, test_data)\n",
    "            pbar.update(len(test_data))\n",
    "        \n",
    "        # Save results\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Predictions saved to {output_path}\")\n",
    "        \n",
    "        # Display a sample of the results\n",
    "        print(\"\\nSample prediction results:\")\n",
    "        for i, result in enumerate(results[:5]):  # Show first 5 results\n",
    "            print(f\"Record {i+1}: High Severity = {result['high_severity']} (Probability: {result['probability']:.2f})\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run predictions using the latest model\n",
    "predictions = run_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4eea8e98214e088e0266c6e2b84f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05db368945349afa53a10c38412d984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Progress:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n",
      "Record 1: High Severity = False (Probability: 0.17)\n",
      "Record 2: High Severity = False (Probability: 0.00)\n",
      "Record 3: High Severity = False (Probability: 0.00)\n",
      "Record 4: High Severity = False (Probability: 0.00)\n",
      "Record 5: High Severity = False (Probability: 0.04)\n"
     ]
    }
   ],
   "source": [
    "# Instead of importing functions, just call them directly\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Run the complete model pipeline: training and prediction.\"\"\"\n",
    "    logger.info(\"Starting wildfire prediction pipeline\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = \"../data/merged_complete.json\"\n",
    "    data = load_data(data_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df = preprocess_data(data)\n",
    "    \n",
    "    # Train model\n",
    "    model, accuracy = train_model(df)\n",
    "    logger.info(f\"Model trained with accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Make predictions on a subset of the data (for demonstration)\n",
    "    sample_data = data[:10]  # Use first 10 records\n",
    "    results = predict(model, sample_data)\n",
    "    \n",
    "    # Print a few predictions\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i, result in enumerate(results[:5]):\n",
    "        print(f\"Record {i+1}: High Severity = {result['high_severity']} (Probability: {result['probability']:.2f})\")\n",
    "    \n",
    "    logger.info(\"Wildfire prediction pipeline completed\")\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "# Run the pipeline\n",
    "model, results = run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
