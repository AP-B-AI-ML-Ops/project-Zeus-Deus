{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### Wildfire Prediction Model Prototype\n",
    "##### Purpose: Test modeling approaches before implementing in production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from src.data_ingestion.weather_data_service import WeatherDataService\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data():\n",
    "    \"\"\"Load NASA FIRMS data from the data directory.\"\"\"\n",
    "    try:\n",
    "        data_path = os.path.join('..', 'data', 'nasa_firms_data.json')\n",
    "        df = pd.read_csv(data_path)\n",
    "        logging.info(f\"Loaded data with shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Data file not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and prepare data for modeling.\"\"\"\n",
    "    # Convert date to datetime\n",
    "    df['acq_date'] = pd.to_datetime(df['acq_date'])\n",
    "       \n",
    "    # Check for and handle missing values\n",
    "    if df.isnull().sum().any():\n",
    "       df = df.dropna()  # or use imputation\n",
    "       \n",
    "    # Filter for US region (if needed)\n",
    "    df = df[(df['latitude'] >= 24) & (df['latitude'] <= 50) & \n",
    "           (df['longitude'] >= -125) & (df['longitude'] <= -66)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create features for wildfire prediction.\"\"\"\n",
    "    # Create grid cells (e.g., 0.1° x 0.1°)\n",
    "    df['lat_grid'] = np.floor(df['latitude'] * 10) / 10\n",
    "    df['lon_grid'] = np.floor(df['longitude'] * 10) / 10\n",
    "\n",
    "    # Aggregate by grid cell and date\n",
    "    grid_counts = df.groupby(['lat_grid', 'lon_grid', 'acq_date']).size().reset_index(name='fire_count')\n",
    "       \n",
    "    # Get unique date range from data\n",
    "    start_date = df['acq_date'].min().strftime('%Y-%m-%d')\n",
    "    end_date = df['acq_date'].max().strftime('%Y-%m-%d')\n",
    "       \n",
    "    # Initialize weather service\n",
    "    weather_service = WeatherDataService()\n",
    "\n",
    "    # Define region bounds for the data (use USA bounds to match your bulk data)\n",
    "    region_bounds = {\n",
    "        'min_lat': 24.0, 'max_lat': 50.0,\n",
    "        'min_lon': -125.0, 'max_lon': -66.0\n",
    "    }\n",
    "       \n",
    "    # Fetch weather data from bulk file\n",
    "    weather_service.fetch_and_store_weather_data(start_date, end_date, region_bounds)\n",
    "       \n",
    "    # Get weather data for our grid cells\n",
    "    df_with_weather = weather_service.get_weather_for_locations(grid_counts)\n",
    "       \n",
    "    # Rename weather columns to match our expected feature names\n",
    "    if df_with_weather is not None:\n",
    "        df_with_weather = df_with_weather.rename(columns={\n",
    "            'temperature_max': 'temperature',\n",
    "            'wind_speed_max': 'wind_speed',\n",
    "            'humidity_max': 'humidity'\n",
    "        })\n",
    "        return df_with_weather\n",
    "    else:\n",
    "        # If we still couldn't get weather data, raise an error\n",
    "        raise ValueError(\"Could not obtain weather data. Please check the bulk_weather_data.csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train a wildfire prediction model.\"\"\"\n",
    "    # Handle class imbalance\n",
    "    # (Most grid cells will not have fires)\n",
    "       \n",
    "    # Initialize and train model\n",
    "    model = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 09:56:45,834 INFO Loaded data with shape: (998, 13)\n",
      "2025-05-13 09:56:45,839 INFO Weather data will be saved to: /workspace/data\n",
      "2025-05-13 09:56:45,866 INFO Available columns in weather data: ['lat_grid', 'lon_grid', 'acq_date', 'fire_count', 'date', 'name', 'datetime', 'tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'preciptype', 'snow', 'snowdepth', 'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'sunrise', 'sunset', 'moonphase', 'conditions', 'description', 'icon', 'stations', 'latitude', 'longitude']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       1.00      1.00      1.00        55\n",
      "\n",
      "    accuracy                           1.00        55\n",
      "   macro avg       1.00      1.00      1.00        55\n",
      "weighted avg       1.00      1.00      1.00        55\n",
      "\n",
      "\n",
      "Feature Importance:\n",
      "         Feature  Importance\n",
      "0    temperature         0.0\n",
      "1  precipitation         0.0\n",
      "2     wind_speed         0.0\n",
      "3       humidity         0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/usr/local/lib/python3.13/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/usr/local/lib/python3.13/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "# Main workflow\n",
    "try:\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    \n",
    "    # Preprocess\n",
    "    df_clean = preprocess_data(df)\n",
    "\n",
    "    df_clean = df_clean.copy()\n",
    "    \n",
    "    # Engineer features with better error handling\n",
    "    try:\n",
    "        # Initialize weather service\n",
    "        weather_service = WeatherDataService()\n",
    "        \n",
    "        # Create grid cells\n",
    "        df_clean['lat_grid'] = np.floor(df_clean['latitude'] * 10) / 10\n",
    "        df_clean['lon_grid'] = np.floor(df_clean['longitude'] * 10) / 10\n",
    "        \n",
    "        # Aggregate by grid cell and date\n",
    "        grid_counts = df_clean.groupby(['lat_grid', 'lon_grid', 'acq_date']).size().reset_index(name='fire_count')\n",
    "        \n",
    "        # Get date range\n",
    "        start_date = df_clean['acq_date'].min().strftime('%Y-%m-%d')\n",
    "        end_date = df_clean['acq_date'].max().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Define region bounds\n",
    "        region_bounds = {\n",
    "            'min_lat': 24.0, 'max_lat': 50.0,\n",
    "            'min_lon': -125.0, 'max_lon': -66.0\n",
    "        }\n",
    "        \n",
    "        # MANUAL FIX: Create a proper weather dataframe directly from the CSV\n",
    "        import os\n",
    "        bulk_weather_file = os.path.join('..', 'data', 'bulk_weather_data.csv')\n",
    "        weather_df = pd.read_csv(bulk_weather_file)\n",
    "        \n",
    "        # Add coordinates for USA data\n",
    "        weather_df['latitude'] = 39.8  # Center of continental US\n",
    "        weather_df['longitude'] = -98.5\n",
    "        \n",
    "        # Convert datetime to proper date format\n",
    "        weather_df['date'] = pd.to_datetime(weather_df['datetime']).dt.date\n",
    "        \n",
    "        # Save to parquet for the service to use\n",
    "        weather_parquet = os.path.join('..', 'data', 'weather_data.parquet')\n",
    "        weather_df.to_parquet(weather_parquet, index=False)\n",
    "        \n",
    "        # Now get weather for locations\n",
    "        df_features = weather_service.get_weather_for_locations(grid_counts)\n",
    "        \n",
    "        logging.info(f\"Available columns in weather data: {df_features.columns.tolist()}\")\n",
    "        \n",
    "        # Ensure we have the right column names\n",
    "        if 'tempmax' in df_features.columns and 'temperature' not in df_features.columns:\n",
    "            df_features['temperature'] = df_features['tempmax']\n",
    "        if 'windspeed' in df_features.columns and 'wind_speed' not in df_features.columns:\n",
    "            df_features['wind_speed'] = df_features['windspeed']\n",
    "        if 'precip' in df_features.columns and 'precipitation' not in df_features.columns:\n",
    "            df_features['precipitation'] = df_features['precip']\n",
    "        if 'humidity' not in df_features.columns and 'humidity_max' in df_features.columns:\n",
    "            df_features['humidity'] = df_features['humidity_max']\n",
    "        \n",
    "        # Define features and target\n",
    "        feature_cols = ['temperature', 'precipitation', 'wind_speed', 'humidity']\n",
    "        \n",
    "        # Check if all required columns exist\n",
    "        missing_cols = [col for col in feature_cols if col not in df_features.columns]\n",
    "        if missing_cols:\n",
    "            logging.warning(f\"Missing columns: {missing_cols}. Adding default values.\")\n",
    "            for col in missing_cols:\n",
    "                if col == 'temperature':\n",
    "                    df_features[col] = 25.0  # Default temperature\n",
    "                elif col == 'precipitation':\n",
    "                    df_features[col] = 0.0   # Default precipitation\n",
    "                elif col == 'wind_speed':\n",
    "                    df_features[col] = 10.0  # Default wind speed\n",
    "                elif col == 'humidity':\n",
    "                    df_features[col] = 50.0  # Default humidity\n",
    "        \n",
    "        # Define features and target\n",
    "        X = df_features[feature_cols]\n",
    "        y = df_features['fire_count'] > 0  # Binary classification: fire or no fire\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        model = train_model(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importance:\")\n",
    "        print(feature_importance)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in feature engineering or modeling: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error in model prototype: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
